{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fd5de9-4382-48a3-83ab-178c0256c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries from Python(gesture)\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97de33d-8a79-4c9d-9373-7dc3a3395c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully\n",
      "Total Images: 20000\n",
      "Classes: {'leapGestRecog\\\\00\\\\01_palm': 0, 'leapGestRecog\\\\00\\\\02_l': 1, 'leapGestRecog\\\\00\\\\03_fist': 2, 'leapGestRecog\\\\00\\\\04_fist_moved': 3, 'leapGestRecog\\\\00\\\\05_thumb': 4, 'leapGestRecog\\\\00\\\\06_index': 5, 'leapGestRecog\\\\00\\\\07_ok': 6, 'leapGestRecog\\\\00\\\\08_palm_moved': 7, 'leapGestRecog\\\\00\\\\09_c': 8, 'leapGestRecog\\\\00\\\\10_down': 9, 'leapGestRecog\\\\01\\\\01_palm': 10, 'leapGestRecog\\\\01\\\\02_l': 11, 'leapGestRecog\\\\01\\\\03_fist': 12, 'leapGestRecog\\\\01\\\\04_fist_moved': 13, 'leapGestRecog\\\\01\\\\05_thumb': 14, 'leapGestRecog\\\\01\\\\06_index': 15, 'leapGestRecog\\\\01\\\\07_ok': 16, 'leapGestRecog\\\\01\\\\08_palm_moved': 17, 'leapGestRecog\\\\01\\\\09_c': 18, 'leapGestRecog\\\\01\\\\10_down': 19, 'leapGestRecog\\\\02\\\\01_palm': 20, 'leapGestRecog\\\\02\\\\02_l': 21, 'leapGestRecog\\\\02\\\\03_fist': 22, 'leapGestRecog\\\\02\\\\04_fist_moved': 23, 'leapGestRecog\\\\02\\\\05_thumb': 24, 'leapGestRecog\\\\02\\\\06_index': 25, 'leapGestRecog\\\\02\\\\07_ok': 26, 'leapGestRecog\\\\02\\\\08_palm_moved': 27, 'leapGestRecog\\\\02\\\\09_c': 28, 'leapGestRecog\\\\02\\\\10_down': 29, 'leapGestRecog\\\\03\\\\01_palm': 30, 'leapGestRecog\\\\03\\\\02_l': 31, 'leapGestRecog\\\\03\\\\03_fist': 32, 'leapGestRecog\\\\03\\\\04_fist_moved': 33, 'leapGestRecog\\\\03\\\\05_thumb': 34, 'leapGestRecog\\\\03\\\\06_index': 35, 'leapGestRecog\\\\03\\\\07_ok': 36, 'leapGestRecog\\\\03\\\\08_palm_moved': 37, 'leapGestRecog\\\\03\\\\09_c': 38, 'leapGestRecog\\\\03\\\\10_down': 39, 'leapGestRecog\\\\04\\\\01_palm': 40, 'leapGestRecog\\\\04\\\\02_l': 41, 'leapGestRecog\\\\04\\\\03_fist': 42, 'leapGestRecog\\\\04\\\\04_fist_moved': 43, 'leapGestRecog\\\\04\\\\05_thumb': 44, 'leapGestRecog\\\\04\\\\06_index': 45, 'leapGestRecog\\\\04\\\\07_ok': 46, 'leapGestRecog\\\\04\\\\08_palm_moved': 47, 'leapGestRecog\\\\04\\\\09_c': 48, 'leapGestRecog\\\\04\\\\10_down': 49, 'leapGestRecog\\\\05\\\\01_palm': 50, 'leapGestRecog\\\\05\\\\02_l': 51, 'leapGestRecog\\\\05\\\\03_fist': 52, 'leapGestRecog\\\\05\\\\04_fist_moved': 53, 'leapGestRecog\\\\05\\\\05_thumb': 54, 'leapGestRecog\\\\05\\\\06_index': 55, 'leapGestRecog\\\\05\\\\07_ok': 56, 'leapGestRecog\\\\05\\\\08_palm_moved': 57, 'leapGestRecog\\\\05\\\\09_c': 58, 'leapGestRecog\\\\05\\\\10_down': 59, 'leapGestRecog\\\\06\\\\01_palm': 60, 'leapGestRecog\\\\06\\\\02_l': 61, 'leapGestRecog\\\\06\\\\03_fist': 62, 'leapGestRecog\\\\06\\\\04_fist_moved': 63, 'leapGestRecog\\\\06\\\\05_thumb': 64, 'leapGestRecog\\\\06\\\\06_index': 65, 'leapGestRecog\\\\06\\\\07_ok': 66, 'leapGestRecog\\\\06\\\\08_palm_moved': 67, 'leapGestRecog\\\\06\\\\09_c': 68, 'leapGestRecog\\\\06\\\\10_down': 69, 'leapGestRecog\\\\07\\\\01_palm': 70, 'leapGestRecog\\\\07\\\\02_l': 71, 'leapGestRecog\\\\07\\\\03_fist': 72, 'leapGestRecog\\\\07\\\\04_fist_moved': 73, 'leapGestRecog\\\\07\\\\05_thumb': 74, 'leapGestRecog\\\\07\\\\06_index': 75, 'leapGestRecog\\\\07\\\\07_ok': 76, 'leapGestRecog\\\\07\\\\08_palm_moved': 77, 'leapGestRecog\\\\07\\\\09_c': 78, 'leapGestRecog\\\\07\\\\10_down': 79, 'leapGestRecog\\\\08\\\\01_palm': 80, 'leapGestRecog\\\\08\\\\02_l': 81, 'leapGestRecog\\\\08\\\\03_fist': 82, 'leapGestRecog\\\\08\\\\04_fist_moved': 83, 'leapGestRecog\\\\08\\\\05_thumb': 84, 'leapGestRecog\\\\08\\\\06_index': 85, 'leapGestRecog\\\\08\\\\07_ok': 86, 'leapGestRecog\\\\08\\\\08_palm_moved': 87, 'leapGestRecog\\\\08\\\\09_c': 88, 'leapGestRecog\\\\08\\\\10_down': 89, 'leapGestRecog\\\\09\\\\01_palm': 90, 'leapGestRecog\\\\09\\\\02_l': 91, 'leapGestRecog\\\\09\\\\03_fist': 92, 'leapGestRecog\\\\09\\\\04_fist_moved': 93, 'leapGestRecog\\\\09\\\\05_thumb': 94, 'leapGestRecog\\\\09\\\\06_index': 95, 'leapGestRecog\\\\09\\\\07_ok': 96, 'leapGestRecog\\\\09\\\\08_palm_moved': 97, 'leapGestRecog\\\\09\\\\09_c': 98, 'leapGestRecog\\\\09\\\\10_down': 99}\n"
     ]
    }
   ],
   "source": [
    "##Load Dataset (leapGestRecog)\n",
    "DATASET_DIR = \"leapGestRecog\"\n",
    "\n",
    "IMG_SIZE = 64  # small size = optimized model\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "label_map = {}\n",
    "label_index = 0\n",
    "\n",
    "for root, dirs, files in os.walk(DATASET_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"):\n",
    "            path = os.path.join(root, file)\n",
    "            label = root.split(\"/\")[-1]\n",
    "\n",
    "            if label not in label_map:\n",
    "                label_map[label] = label_index\n",
    "                label_index += 1\n",
    "\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            X.append(img)\n",
    "            y.append(label_map[label])\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = to_categorical(y, num_classes=len(label_map))\n",
    "\n",
    "print(\"Dataset Loaded Successfully\")\n",
    "print(\"Total Images:\", len(X))\n",
    "print(\"Classes:\", label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397465e7-fc13-4ef6-831b-038daf8cfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428383ec-c298-4587-9804-8960a9536a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c81e38-83c5-47a3-bf40-8c377fc86e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 62, 62, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 29, 29, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 12, 12, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               12900     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 696,420\n",
      "Trainable params: 695,972\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##Build CNN Model (convolutional Neural Network)\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(len(label_map), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=Adam(0.0005),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78ee5a9-44aa-480f-a4ad-a3f415f3c834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 34s 66ms/step - loss: 3.7367 - accuracy: 0.1433 - val_loss: 1.5024 - val_accuracy: 0.5972\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.6337 - accuracy: 0.5290 - val_loss: 0.2796 - val_accuracy: 0.9147\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7611 - accuracy: 0.7617 - val_loss: 0.1107 - val_accuracy: 0.9628\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.4577 - accuracy: 0.8516 - val_loss: 0.0546 - val_accuracy: 0.9805\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.2876 - accuracy: 0.9016 - val_loss: 0.0596 - val_accuracy: 0.9758\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.2331 - accuracy: 0.9211 - val_loss: 0.0563 - val_accuracy: 0.9772\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.1955 - accuracy: 0.9327 - val_loss: 0.0608 - val_accuracy: 0.9755\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.1700 - accuracy: 0.9386 - val_loss: 0.0187 - val_accuracy: 0.9883\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.1422 - accuracy: 0.9489 - val_loss: 0.1792 - val_accuracy: 0.9463\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 34s 67ms/step - loss: 0.1395 - accuracy: 0.9491 - val_loss: 0.0673 - val_accuracy: 0.9743\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.1218 - accuracy: 0.9550 - val_loss: 0.0251 - val_accuracy: 0.9875\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.1097 - accuracy: 0.9593 - val_loss: 0.0182 - val_accuracy: 0.9902\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.1014 - accuracy: 0.9636 - val_loss: 0.1360 - val_accuracy: 0.9615\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.1007 - accuracy: 0.9624 - val_loss: 0.0150 - val_accuracy: 0.9898\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 35s 71ms/step - loss: 0.0909 - accuracy: 0.9651 - val_loss: 0.0264 - val_accuracy: 0.9875\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.0969 - accuracy: 0.9659 - val_loss: 0.0179 - val_accuracy: 0.9885\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.0918 - accuracy: 0.9666 - val_loss: 0.0169 - val_accuracy: 0.9902\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.0907 - accuracy: 0.9657 - val_loss: 0.0231 - val_accuracy: 0.9880\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.0763 - accuracy: 0.9714 - val_loss: 0.1439 - val_accuracy: 0.9610\n"
     ]
    }
   ],
   "source": [
    "##Train Model\n",
    "checkpoint = ModelCheckpoint(\"gesture_model.h5\", save_best_only=True, monitor=\"val_accuracy\")\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740fce1c-d68b-4507-91eb-17f06fa8bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 20ms/step - loss: 0.0150 - accuracy: 0.9898\n",
      "Test Accuracy: 0.9897500276565552\n"
     ]
    }
   ],
   "source": [
    "##Evaluate Model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "026c8e40-b705-4107-95a4-689b1efbdf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & Labels Saved Successfully\n"
     ]
    }
   ],
   "source": [
    "##Save Model + Label Map\n",
    "model.save(\"gesture_model.h5\")\n",
    "\n",
    "with open(\"label_map.json\", \"w\") as f:\n",
    "    json.dump(label_map, f)\n",
    "\n",
    "print(\"Model & Labels Saved Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f066ded3-5a49-41dc-a468-a25c2acdca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\maury\\AppData\\Local\\Temp\\tmpox67_9iq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\maury\\AppData\\Local\\Temp\\tmpox67_9iq\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized model saved\n"
     ]
    }
   ],
   "source": [
    "##MODEL SIZE OPTIMIZATION\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"gesture_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Optimized model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80888c9-2c16-411e-aabd-43841937515a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gesture)",
   "language": "python",
   "name": "gesture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
